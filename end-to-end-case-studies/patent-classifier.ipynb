{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gradient 2.0.6 requires attrs<=19, but you have attrs 23.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "! pip install -q datasets==2.20.0 \\\n",
    "                 accelerate==0.33.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from transformers import create_optimizer\n",
    "\n",
    "from transformers import AutoTokenizer, DataCollatorWithPadding\n",
    "from transformers import TFAutoModelForSequenceClassification\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset, load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method\n",
    "\n",
    "Just as with transfer learning with images, pretrained BERT models can be fine-tuned by:\n",
    "\n",
    "- Importing a pretrained model from HuggingFace and attaching a classifier head.\n",
    "- Then, we freeze the base BERT model and finetune the dense layer.\n",
    "- Finally, we unfreeze the base BERT model and finetune the entire model.\n",
    "\n",
    "The finetuned model is now ready for inference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9288458167c4073af71e52b978d6948",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/8.61M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b94ef84297424fb6aec4e86921cf895b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.74M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d461d4aa2a544b49605fb05d10cc265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading data:   0%|          | 0.00/1.72M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b822475af403462da9317dcd41602831",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "caa12cadd67441d3b2105fc79acf1f90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5936568e7941455f8ce8b828fe11ff33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "ds = load_dataset(\"ccdv/patent-classification\", \"abstract\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/huggingface_hub/file_download.py:1150: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "919335aec3cf49a8a47605a9c1b288dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "504ae77a0b3b4d1285fc50bdb3d62740",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61a426b5e9d948efa5cdc7876c1b8ea4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "19270f46656c4d3988b0f3281ca0044c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"distilbert/distilbert-base-uncased\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e614e6c78c9e4cb78c32531d1d7294ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/25000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7173ffd08d8942679590a91e272bda90",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8a4cb8f9224bffbe3053324e82a08a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenized_dataset = ds.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 25000\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text', 'label', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 5000\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer, return_tensors=\"tf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "id2label = {\n",
    "    0: \"Human Necessities\", \n",
    "    1: \"Performing Operations; Transporting\",\n",
    "    2: \"Chemistry; Metallurgy\",\n",
    "    3: \"Textiles; Paper\",\n",
    "    4: \"Fixed Constructions\",\n",
    "    5: \"Mechanical Engineering; Lightning; Heating; Weapons; Blasting\",\n",
    "    6: \"Physics\",\n",
    "    7: \"Electricity\",\n",
    "    8: \"General tagging of new or cross-sectional technology\"\n",
    "}\n",
    "\n",
    "label2id = { v: k for k, v in id2label.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 24\n",
    "num_epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-27 04:52:30.097522: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-27 04:52:30.136796: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-27 04:52:30.137049: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-27 04:52:30.140177: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-27 04:52:30.140488: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-27 04:52:30.140726: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-27 04:52:31.999265: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-27 04:52:31.999524: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-27 04:52:31.999703: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:901] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2024-08-27 04:52:31.999828: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1929] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 15526 MB memory:  -> device: 0, name: Quadro P5000, pci bus id: 0000:00:05.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "batches_per_epoch = len(tokenized_dataset[\"train\"]) // batch_size\n",
    "total_train_steps = int(batches_per_epoch * num_epochs)\n",
    "\n",
    "optimizer, schedule = create_optimizer(\n",
    "    init_lr=2e-5,\n",
    "    min_lr_ratio=0.001,\n",
    "    num_warmup_steps=0,\n",
    "    num_train_steps=total_train_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0608364985c4437ba84c6bf5f2ab48c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFDistilBertForSequenceClassification: ['vocab_layer_norm.bias', 'vocab_projector.bias', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_transform.weight']\n",
      "- This IS expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights or buffers of the TF 2.0 model TFDistilBertForSequenceClassification were not initialized from the PyTorch model and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model = TFAutoModelForSequenceClassification.from_pretrained(\n",
    "    \"distilbert/distilbert-base-uncased\",\n",
    "    num_labels=9,\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMa  multiple                  66362880  \n",
      " inLayer)                                                        \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  6921      \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66960393 (255.43 MB)\n",
      "Trainable params: 66960393 (255.43 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-Tune"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Freeze the base BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMa  multiple                  66362880  \n",
      " inLayer)                                                        \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  6921      \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66960393 (255.43 MB)\n",
      "Trainable params: 597513 (2.28 MB)\n",
      "Non-trainable params: 66362880 (253.15 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=optimizer,\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use two callbacks - model checkpointing when best accuracy is observed and early stopping if validation accuracy does not improve for 4 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"best_model\",\n",
    "    monitor=\"val_accuracy\",\n",
    "    mode=\"max\",\n",
    "    save_best_only=True,\n",
    "    save_weights_only=True\n",
    ")\n",
    "\n",
    "earlystopping = tf.keras.callbacks.EarlyStopping(\n",
    "    patience=4,\n",
    "    monitor=\"val_accuracy\",\n",
    "    restore_best_weights=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "tf_train_set = model.prepare_tf_dataset(\n",
    "    tokenized_dataset['train'],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_val_set = model.prepare_tf_dataset(\n",
    "    tokenized_dataset['validation'],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-08-27 04:53:55.478404: I external/local_xla/xla/service/service.cc:168] XLA service 0x33496900 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "2024-08-27 04:53:55.478468: I external/local_xla/xla/service/service.cc:176]   StreamExecutor device (0): Quadro P5000, Compute Capability 6.1\n",
      "2024-08-27 04:53:55.485104: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:269] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "2024-08-27 04:53:56.325699: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:454] Loaded cuDNN version 8902\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "I0000 00:00:1724734436.442683     228 device_compiler.h:186] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1562/1562 [==============================] - 258s 158ms/step - loss: 1.6562 - accuracy: 0.4214 - val_loss: 1.4048 - val_accuracy: 0.5178\n",
      "Epoch 2/50\n",
      "1562/1562 [==============================] - 248s 159ms/step - loss: 1.3558 - accuracy: 0.5173 - val_loss: 1.2993 - val_accuracy: 0.5521\n",
      "Epoch 3/50\n",
      "1562/1562 [==============================] - 247s 158ms/step - loss: 1.2929 - accuracy: 0.5355 - val_loss: 1.2588 - val_accuracy: 0.5599\n",
      "Epoch 4/50\n",
      "1562/1562 [==============================] - 248s 159ms/step - loss: 1.2585 - accuracy: 0.5491 - val_loss: 1.2312 - val_accuracy: 0.5663\n",
      "Epoch 5/50\n",
      "1562/1562 [==============================] - 247s 158ms/step - loss: 1.2399 - accuracy: 0.5575 - val_loss: 1.2155 - val_accuracy: 0.5711\n",
      "Epoch 6/50\n",
      "1562/1562 [==============================] - 247s 158ms/step - loss: 1.2229 - accuracy: 0.5595 - val_loss: 1.2015 - val_accuracy: 0.5707\n",
      "Epoch 7/50\n",
      "1562/1562 [==============================] - 247s 158ms/step - loss: 1.2128 - accuracy: 0.5665 - val_loss: 1.1895 - val_accuracy: 0.5767\n",
      "Epoch 8/50\n",
      "1562/1562 [==============================] - 248s 158ms/step - loss: 1.2029 - accuracy: 0.5677 - val_loss: 1.1808 - val_accuracy: 0.5803\n",
      "Epoch 9/50\n",
      "1562/1562 [==============================] - 246s 157ms/step - loss: 1.1881 - accuracy: 0.5739 - val_loss: 1.1749 - val_accuracy: 0.5797\n",
      "Epoch 10/50\n",
      "1562/1562 [==============================] - 248s 159ms/step - loss: 1.1857 - accuracy: 0.5754 - val_loss: 1.1662 - val_accuracy: 0.5829\n",
      "Epoch 11/50\n",
      "1562/1562 [==============================] - 248s 159ms/step - loss: 1.1788 - accuracy: 0.5771 - val_loss: 1.1583 - val_accuracy: 0.5871\n",
      "Epoch 12/50\n",
      "1562/1562 [==============================] - 249s 159ms/step - loss: 1.1707 - accuracy: 0.5804 - val_loss: 1.1523 - val_accuracy: 0.5903\n",
      "Epoch 13/50\n",
      "1562/1562 [==============================] - 247s 158ms/step - loss: 1.1644 - accuracy: 0.5798 - val_loss: 1.1509 - val_accuracy: 0.5859\n",
      "Epoch 14/50\n",
      "1562/1562 [==============================] - 247s 158ms/step - loss: 1.1607 - accuracy: 0.5821 - val_loss: 1.1458 - val_accuracy: 0.5911\n",
      "Epoch 15/50\n",
      "1562/1562 [==============================] - 247s 158ms/step - loss: 1.1563 - accuracy: 0.5851 - val_loss: 1.1413 - val_accuracy: 0.5901\n",
      "Epoch 16/50\n",
      "1562/1562 [==============================] - 249s 159ms/step - loss: 1.1536 - accuracy: 0.5842 - val_loss: 1.1382 - val_accuracy: 0.5942\n",
      "Epoch 17/50\n",
      "1562/1562 [==============================] - 246s 157ms/step - loss: 1.1479 - accuracy: 0.5844 - val_loss: 1.1340 - val_accuracy: 0.5929\n",
      "Epoch 18/50\n",
      "1562/1562 [==============================] - 246s 158ms/step - loss: 1.1456 - accuracy: 0.5859 - val_loss: 1.1341 - val_accuracy: 0.5913\n",
      "Epoch 19/50\n",
      "1562/1562 [==============================] - 246s 158ms/step - loss: 1.1441 - accuracy: 0.5878 - val_loss: 1.1303 - val_accuracy: 0.5935\n",
      "Epoch 20/50\n",
      "1562/1562 [==============================] - 250s 160ms/step - loss: 1.1418 - accuracy: 0.5860 - val_loss: 1.1268 - val_accuracy: 0.5968\n",
      "Epoch 21/50\n",
      "1562/1562 [==============================] - 247s 158ms/step - loss: 1.1388 - accuracy: 0.5884 - val_loss: 1.1250 - val_accuracy: 0.5968\n",
      "Epoch 22/50\n",
      "1562/1562 [==============================] - 248s 159ms/step - loss: 1.1367 - accuracy: 0.5905 - val_loss: 1.1237 - val_accuracy: 0.5980\n",
      "Epoch 23/50\n",
      "1562/1562 [==============================] - 251s 161ms/step - loss: 1.1350 - accuracy: 0.5895 - val_loss: 1.1221 - val_accuracy: 0.5976\n",
      "Epoch 24/50\n",
      "1562/1562 [==============================] - 246s 158ms/step - loss: 1.1342 - accuracy: 0.5911 - val_loss: 1.1212 - val_accuracy: 0.5970\n",
      "Epoch 25/50\n",
      "1562/1562 [==============================] - 247s 158ms/step - loss: 1.1328 - accuracy: 0.5915 - val_loss: 1.1189 - val_accuracy: 0.5998\n",
      "Epoch 26/50\n",
      "1562/1562 [==============================] - 246s 158ms/step - loss: 1.1301 - accuracy: 0.5931 - val_loss: 1.1181 - val_accuracy: 0.5982\n",
      "Epoch 27/50\n",
      "1562/1562 [==============================] - 247s 158ms/step - loss: 1.1299 - accuracy: 0.5907 - val_loss: 1.1166 - val_accuracy: 0.6008\n",
      "Epoch 28/50\n",
      "1562/1562 [==============================] - 251s 161ms/step - loss: 1.1291 - accuracy: 0.5919 - val_loss: 1.1161 - val_accuracy: 0.6014\n",
      "Epoch 29/50\n",
      "1562/1562 [==============================] - 247s 158ms/step - loss: 1.1272 - accuracy: 0.5919 - val_loss: 1.1154 - val_accuracy: 0.5992\n",
      "Epoch 30/50\n",
      "1562/1562 [==============================] - 247s 158ms/step - loss: 1.1254 - accuracy: 0.5941 - val_loss: 1.1145 - val_accuracy: 0.5994\n",
      "Epoch 31/50\n",
      "1562/1562 [==============================] - 247s 158ms/step - loss: 1.1255 - accuracy: 0.5943 - val_loss: 1.1137 - val_accuracy: 0.5994\n",
      "Epoch 32/50\n",
      "1562/1562 [==============================] - 247s 158ms/step - loss: 1.1242 - accuracy: 0.5964 - val_loss: 1.1150 - val_accuracy: 0.5980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f2cd1ae2810>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    tf_train_set,\n",
    "    validation_data=tf_val_set,\n",
    "    epochs=50,\n",
    "    callbacks=[checkpoint, earlystopping]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfreeze the base BERT model and continue training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[0].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"tf_distil_bert_for_sequence_classification\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " distilbert (TFDistilBertMa  multiple                  66362880  \n",
      " inLayer)                                                        \n",
      "                                                                 \n",
      " pre_classifier (Dense)      multiple                  590592    \n",
      "                                                                 \n",
      " classifier (Dense)          multiple                  6921      \n",
      "                                                                 \n",
      " dropout_19 (Dropout)        multiple                  0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 66960393 (255.43 MB)\n",
      "Trainable params: 66960393 (255.43 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "1562/1562 [==============================] - 245s 157ms/step - loss: 1.1269 - accuracy: 0.5944 - val_loss: 1.1153 - val_accuracy: 0.5996\n",
      "Epoch 2/50\n",
      "1562/1562 [==============================] - 247s 158ms/step - loss: 1.1268 - accuracy: 0.5929 - val_loss: 1.1166 - val_accuracy: 0.5992\n",
      "Epoch 3/50\n",
      "1562/1562 [==============================] - 245s 157ms/step - loss: 1.1261 - accuracy: 0.5925 - val_loss: 1.1168 - val_accuracy: 0.5990\n",
      "Epoch 4/50\n",
      "1562/1562 [==============================] - 251s 161ms/step - loss: 1.1261 - accuracy: 0.5945 - val_loss: 1.1156 - val_accuracy: 0.5996\n",
      "Epoch 5/50\n",
      "1562/1562 [==============================] - 248s 159ms/step - loss: 1.1262 - accuracy: 0.5924 - val_loss: 1.1159 - val_accuracy: 0.5994\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7f2c443b6c10>"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    tf_train_set,\n",
    "    validation_data=tf_val_set,\n",
    "    epochs=50,\n",
    "    callbacks=[checkpoint, earlystopping]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_test_set = model.prepare_tf_dataset(\n",
    "    tokenized_dataset['test'],\n",
    "    shuffle=True,\n",
    "    batch_size=16,\n",
    "    collate_fn=data_collator,\n",
    "    tokenizer=tokenizer\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312/312 [==============================] - 36s 115ms/step - loss: 1.1366 - accuracy: 0.5917\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.1365915536880493, 0.5917468070983887]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(tf_test_set)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
